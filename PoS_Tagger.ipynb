{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DEPENDENCIES ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\marci\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "import pyconll\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.layers import Bidirectional, Embedding, LSTM, Dense, Input, TimeDistributed\n",
    "from keras.models import Model\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from keras.callbacks import CSVLogger\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### VERSION ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow  2.13.0\n",
      "Keras  2.13.1\n"
     ]
    }
   ],
   "source": [
    "print(\"Tensorflow \", tf.__version__)\n",
    "print(\"Keras \", keras.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensorflow  2.13.0\n",
    "\n",
    "Keras  2.13.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LANGUAGE AND TRAINING CHOICE ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "################### DO YOU WANT TO RETRAIN THE MODEL? True/False #######################\n",
    "\n",
    "retrain = True\n",
    "\n",
    "########################################################################################\n",
    "\n",
    "\n",
    "my_train_conll_file_location = \"languages/en_ewt-ud-train.conllu\"\n",
    "my_dev_conll_file_location = \"languages/en_ewt-ud-dev.conllu\"\n",
    "my_test_conll_file_location = \"languages/en_ewt-ud-test.conllu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LOAD DATA ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sentence in train:  Antichrist John Lennon wanted to compete with Jesus Christ, and so he grew a beard and started to make a bogus role of Christ together with Yoko Ono at the Amsterdam Hilton hotel proclaiming \"Peace\", being then when he was visited by the Canadian journalist who ridiculized and admonished him wanting to know about what Lennon meant when he wrote in the lyrics of \"The ballad of John and Yoko\": \"the way things are going, they're going to crucify me...\", The CURSE OF GOD upon John Lennon carried on with all type of miseries and distresses which made Lennon give the interview to the \"Rolling Stone\" magazine (today condensed in the \"Lennon remembers\" book) where he speaks about how bad thing were going for him blaming \"whatever is up there\" for it (referring to God).\n",
      "Sentence length:  160\n",
      "Sentence removed, too long\n",
      "\n",
      "Sentence in train:  my name is Josalyn Leainne Creek and i'm 19 years old and i graduated from high in May adn of this year and i love animals and i'm great at working and i had drama in my last job which was mcallister's deli and i', a mormon and i hvae changed to be a better person that pwople are wanting to hirier and i'm willing to work whenever you need me but i go to church on every sunday and i have a doctor's appointment this thursday at 2;30 and i hope you h=guys hirier me because i work very hard and i dont care about how much you guys pay and i've been looking for a job ever since i have graduated and i hate drama\n",
      "Sentence length:  143\n",
      "Sentence removed, too long\n"
     ]
    }
   ],
   "source": [
    "train_file = pyconll.load_from_file(my_train_conll_file_location)\n",
    "test_file = pyconll.load_from_file(my_dev_conll_file_location)\n",
    "dev_file = pyconll.load_from_file(my_test_conll_file_location)\n",
    "\n",
    "#print the first sentence\n",
    "#print(train_file[0].text)\n",
    "\n",
    "#look for sentences with more than 128 words or empty words\n",
    "for sentence in train_file:\n",
    "    if len(sentence) > 128:\n",
    "        print(\"\\nSentence in train: \", sentence.text)\n",
    "        print(\"Sentence length: \", len(sentence))\n",
    "        train_file.remove(sentence)\n",
    "        print(\"Sentence removed, too long\")\n",
    "    \n",
    "    for word in sentence:\n",
    "        if word.form == None:\n",
    "            train_file.remove(sentence)\n",
    "            # print(\"Sentence removed, None existed\")\n",
    "\n",
    "for sentence in test_file:\n",
    "    if len(sentence) > 128:\n",
    "        print(\"\\nSentence in test: \", sentence.text)\n",
    "        print(\"Sentence length: \", len(sentence))\n",
    "        test_file.remove(sentence)\n",
    "        print(\"Sentence removed, too long\")\n",
    "    \n",
    "    for word in sentence:\n",
    "        if word.form == None:\n",
    "            test_file.remove(sentence)\n",
    "            # print(\"Sentence removed, None existed\")\n",
    "\n",
    "for sentence in dev_file:\n",
    "    if len(sentence) > 128:\n",
    "        print(\"\\nSentence in dev: \", sentence.text)\n",
    "        print(\"Sentence length: \", len(sentence))\n",
    "        dev_file.remove(sentence)\n",
    "        print(\"Sentence removed, too long\")\n",
    "    \n",
    "    for word in sentence:\n",
    "        if word.form == None:\n",
    "            dev_file.remove(sentence)\n",
    "            # print(\"Sentence removed, None existed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SPLIT DATASETS AND CREATE LIST OF PoS TAGS ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POS tags:  {'PUNCT', 'VERB', 'ADP', 'SCONJ', 'INTJ', 'SYM', 'PART', 'AUX', 'DET', 'ADV', 'X', 'NUM', None, 'ADJ', 'PROPN', 'PRON', 'CCONJ', 'NOUN'}\n",
      "POS tag to index:  {'PUNCT': 0, 'VERB': 1, 'ADP': 2, 'SCONJ': 3, 'INTJ': 4, 'SYM': 5, 'PART': 6, 'AUX': 7, 'DET': 8, 'ADV': 9, 'X': 10, 'NUM': 11, None: 12, 'ADJ': 13, 'PROPN': 14, 'PRON': 15, 'CCONJ': 16, 'NOUN': 17}\n"
     ]
    }
   ],
   "source": [
    "# X_train, y_train: Training data\n",
    "# X_dev, y_dev: Development (validation) data\n",
    "# X_test, y_test: Test data\n",
    "\n",
    "# create a list of sentences\n",
    "X_train = []\n",
    "y_train = []\n",
    "for sentence in train_file:\n",
    "    X_train.append([word.form for word in sentence])\n",
    "    y_train.append([word.upos for word in sentence])\n",
    "    \n",
    "X_dev = []\n",
    "y_dev = []\n",
    "for sentence in dev_file:\n",
    "    X_dev.append([word.form for word in sentence])\n",
    "    y_dev.append([word.upos for word in sentence])\n",
    "    \n",
    "X_test = []\n",
    "y_test = []\n",
    "for sentence in test_file:\n",
    "    X_test.append([word.form for word in sentence])\n",
    "    y_test.append([word.upos for word in sentence])\n",
    "    \n",
    "# Create a list of all part-of-speech tags\n",
    "pos_tags = set()\n",
    "for sentence in y_train:\n",
    "    for tag in sentence:\n",
    "        pos_tags.add(tag)\n",
    "num_pos_tags = len(pos_tags)\n",
    "\n",
    "print(\"POS tags: \", pos_tags)\n",
    "\n",
    "# Create a mapping from part-of-speech tags to integers\n",
    "pos_tag_names = list(pos_tags)\n",
    "pos_tag_to_idx = {t: i for i, t in enumerate(pos_tag_names)}\n",
    "\n",
    "print(\"POS tag to index: \", pos_tag_to_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TOKENIZER ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a world-level tokenizer\n",
    "tokenizer = Tokenizer(oov_token=\"<unk>\", filters='\\t\\n')\n",
    "\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "vocabulary_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "max_sequence_length = 128\n",
    "\n",
    "X_train_OHE = tokenizer.texts_to_sequences(X_train)\n",
    "X_train_OHE = pad_sequences(X_train_OHE, padding='post', maxlen=max_sequence_length)\n",
    "\n",
    "X_dev_OHE = tokenizer.texts_to_sequences(X_dev)\n",
    "X_dev_OHE = pad_sequences(X_dev_OHE, padding='post', maxlen=max_sequence_length)\n",
    "\n",
    "X_test_OHE = tokenizer.texts_to_sequences(X_test)\n",
    "X_test_OHE = pad_sequences(X_test_OHE, padding='post', maxlen=max_sequence_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TRANSFORMING LABELS ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(pos_tag_names)\n",
    "\n",
    "# Convert the labels from strings to integers\n",
    "y_train_int = [label_encoder.transform(sequence) for sequence in y_train]\n",
    "y_dev_int = [label_encoder.transform(sequence) for sequence in y_dev]\n",
    "y_test_int = [label_encoder.transform(sequence) for sequence in y_test]\n",
    "\n",
    "y_train_int = pad_sequences(y_train_int, maxlen=max_sequence_length, padding='post', value=0)\n",
    "y_dev_int = pad_sequences(y_dev_int, maxlen=max_sequence_length, padding='post', value=0)\n",
    "y_test_int = pad_sequences(y_test_int, maxlen=max_sequence_length, padding='post', value=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ONE-HOT ENCODING ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the integer labels to one-hot encodings\n",
    "vector_train = [to_categorical(sequence, num_classes=num_pos_tags) for sequence in y_train_int]\n",
    "vector_dev = [to_categorical(sequence, num_classes=num_pos_tags) for sequence in y_dev_int]\n",
    "vector_test = [to_categorical(sequence, num_classes=num_pos_tags) for sequence in y_test_int]\n",
    "\n",
    "vector_train = np.array(vector_train)\n",
    "vector_dev = np.array(vector_dev)\n",
    "vector_test = np.array(vector_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MODEL TRAINING ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 128)]             0         \n",
      "                                                                 \n",
      " embedding_1 (Embedding)     (None, 128, 128)          2187136   \n",
      "                                                                 \n",
      " bidirectional_1 (Bidirecti  (None, 128, 128)          98816     \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " time_distributed_1 (TimeDi  (None, 128, 18)           2322      \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2288274 (8.73 MB)\n",
      "Trainable params: 2288274 (8.73 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/5\n",
      "196/196 [==============================] - 66s 280ms/step - loss: 1.4209 - accuracy: 0.6074 - val_loss: 0.4684 - val_accuracy: 0.8765\n",
      "Epoch 2/5\n",
      "196/196 [==============================] - 53s 271ms/step - loss: 0.2532 - accuracy: 0.9322 - val_loss: 0.2981 - val_accuracy: 0.9130\n",
      "Epoch 3/5\n",
      "196/196 [==============================] - 55s 281ms/step - loss: 0.1366 - accuracy: 0.9621 - val_loss: 0.2765 - val_accuracy: 0.9154\n",
      "Epoch 4/5\n",
      "196/196 [==============================] - 57s 291ms/step - loss: 0.0969 - accuracy: 0.9721 - val_loss: 0.2747 - val_accuracy: 0.9173\n",
      "Epoch 5/5\n",
      "196/196 [==============================] - 52s 267ms/step - loss: 0.0744 - accuracy: 0.9790 - val_loss: 0.2784 - val_accuracy: 0.9176\n",
      "INFO:tensorflow:Assets written to: model.English\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model.English\\assets\n"
     ]
    }
   ],
   "source": [
    "if retrain == True:\n",
    "    embedding_size = 128\n",
    "\n",
    "    input_layer = Input(shape=(max_sequence_length,))\n",
    "    embedding_layer = Embedding(input_dim=vocabulary_size, output_dim=embedding_size, mask_zero=True)(input_layer)\n",
    "    bidirectional_lstm = Bidirectional(LSTM(units=64, return_sequences=True))(embedding_layer)\n",
    "    output_layer = TimeDistributed(Dense(num_pos_tags, activation='softmax'))(bidirectional_lstm)\n",
    "\n",
    "    model = Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    print(model.summary())\n",
    "\n",
    "    # Save the model's performance\n",
    "    csv_logger = CSVLogger('training_' + 'English' + '.log')\n",
    "\n",
    "    model.fit(X_train_OHE, vector_train, batch_size=64, epochs=5, validation_data=(X_dev_OHE, vector_dev), callbacks=[csv_logger])\n",
    "\n",
    "    # Save the model\n",
    "    model.save('model.' + 'English')\n",
    "else:\n",
    "    # Load the model\n",
    "    model = keras.models.load_model('model.' + 'English')\n",
    "    print(model.summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EVALUATE MODEL ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 3s 40ms/step - loss: 0.3003 - accuracy: 0.9157\n",
      "Loss: 30.03, Accuracy: 91.57\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test_OHE, vector_test, verbose=1)\n",
    "print('Loss: %.2f, Accuracy: %.2f' % (loss*100,accuracy*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PoS TAGGING FUNCTION FOR UNKNOWN SENTENCES ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_tag_sentence(sentence, tokenizer, model, max_sequence_length):\n",
    "    # Convert sentence to a list of words and punctuation symbols\n",
    "    sentences = nltk.word_tokenize(sentence)\n",
    "    print(sentences)\n",
    "\n",
    "    X = tokenizer.texts_to_sequences(sentences)\n",
    "\n",
    "    X = pad_sequences(X, maxlen=max_sequence_length, value=0)\n",
    "\n",
    "    predictions = model.predict(X)\n",
    "\n",
    "    # Convert predictions to part-of-speech tags\n",
    "    predicted_tags = np.argmax(predictions, axis=2)\n",
    "    \n",
    "    # Remove padding\n",
    "    values = predicted_tags[:, -1]\n",
    "    tag_names = label_encoder.inverse_transform(values)\n",
    "\n",
    "    return tag_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TEST ON A SAMPLE SENTENCE ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', \"'m\", 'very', 'happy', '#', 'today', ',', 'because', '!', 'it', \"'s\", 'sunny', 'outside', '.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 3s 3s/step\n",
      "['PRON' 'AUX' 'ADV' 'ADJ' 'SYM' 'NOUN' 'PUNCT' 'SCONJ' 'PUNCT' 'PRON'\n",
      " 'PART' 'ADJ' 'ADP' 'PUNCT']\n"
     ]
    }
   ],
   "source": [
    "# Dictionary of sentences in each language\n",
    "sentence = (\"I'm very happy # today, because! it's sunny outside.\")\n",
    "\n",
    "result = pos_tag_sentence(sentence, tokenizer, model, max_sequence_length)\n",
    "\n",
    "print(result)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
